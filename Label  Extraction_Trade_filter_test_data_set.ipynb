{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e910522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nick/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/nick/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### ignore warning \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "import spacy \n",
    "import nltk\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer ,word_tokenize\n",
    "from nltk.stem import PorterStemmer , WordNetLemmatizer\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import everygrams\n",
    "\n",
    "# nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import everygrams\n",
    "\n",
    "# display  max row \n",
    "#pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "## tqdm \n",
    "from tqdm._tqdm_notebook import tqdm_notebook,tqdm\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "from spacy import displacy\n",
    "nlp1 = spacy.load('en_core_web_sm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90339f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 25)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_test = pd.read_csv('./new_test_set_added.csv')\n",
    "df_new_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50bac60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cae3ff4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226ecd5b1687410db4cdd7a59fdf8717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8899356a0a4d9ea70c183b7aed976c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce0b3f82324453a87384c764f658060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13d721547594ec5b3851bb116c1d8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prep_data(data) :\n",
    "    word_domain = []\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    stop_word  = stopwords.words('english')\n",
    "    #data = data_test\n",
    "    data = data.strip()\n",
    "    # optional  base on inspection  \n",
    "    #data  =  data.lower()\n",
    "    ########################### \n",
    "    result = string.punctuation\n",
    "    ### Clean HTML Tacontents = df_clean.content.to_list()\n",
    "    data = re.sub(r'\\n','',data)\n",
    "    data = re.sub(r'\\xa0','',data)\n",
    "    data = re.sub(r\"\\\\\",\"\",data)\n",
    "    data = re.sub(r'<li>','',data)\n",
    "    data = re.sub(r'</li>','',data)\n",
    "    data = re.sub(r'</ul>','',data)\n",
    "    data = re.sub(r'<td>','',data)\n",
    "    data = re.sub(r'</td>','',data)\n",
    "    \n",
    "    \n",
    "    data= re.sub(r'\\u200b', '', data)\n",
    "    data = re.sub(r'[ๆฯ!#$&%\\\"\\'()*+,-./:;<=>?@\\[\\]\\\\^_`{}|~]',' ', data)\n",
    "    data = re.sub(r'\\d',' ', data)\n",
    "    data = re.sub(r'\\n', ' ', data)\n",
    "    #data = re.sub(r'[^a-zA-Z\\s]', '', data, re.I|re.A)\n",
    "    data = re.sub(r'・','',data)\n",
    "    \n",
    "\n",
    "   \n",
    "    lst_pun = [i for i in result] + ['•'] +['“','”','’']+['©']+['£']+['—'] +['-']\n",
    "    data = data.replace('|',' ')\n",
    "    data = data.replace('-', ' ')\n",
    "    data = data.replace('|****|',' ')\n",
    "    data = wpt.tokenize(data)\n",
    "    data =  ' '.join( i for i in data if i not in lst_pun and i.isnumeric() == False ) \n",
    "    data  =  nltk.tokenize.word_tokenize(data)\n",
    "    \n",
    "    data =  [i for i in data if i not in  stop_word ]\n",
    "    data   = ' '.join(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def ner_process(sentens_string) :\n",
    "    doc1 = nlp1(sentens_string) \n",
    "    sentenss = []\n",
    "    for sr in  doc1 :\n",
    "        if sr.ent_iob_  == 'O' :\n",
    "               sentenss.append(str(sr)) \n",
    "    return ' '.join(sentenss)  \n",
    "    #data  = list(everygrams(data, 1, 3))\n",
    "    \n",
    "def noun_only(x):\n",
    "    x  = word_tokenize(x)\n",
    "    pos_comment = nltk.pos_tag(x)\n",
    "    filtered = [word[0] for word in pos_comment if word[1] in ['NN']]\n",
    "    # to filter both noun and verbs\n",
    "    #filtered = [word[0] for word in pos_comment if word[1] in ['NN','VB', 'VBD', 'VBG', 'VBN', 'VBZ']]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def count_vector(key_word,content) :\n",
    "    key_word = [' '.join(key_word)]\n",
    "    print(len(key_word))\n",
    "    corpus =  key_word + content\n",
    "    #corpus =  econ_all2 + xx_list\n",
    "    #corpus_str = [' '.join(corpus)]\n",
    "    #print(len(corpus)\n",
    "    cvec = CountVectorizer(analyzer = lambda x:x.split(' ')) #\n",
    "    cvec.fit_transform(corpus)\n",
    "    print(len(cvec.vocabulary_))\n",
    "\n",
    "    train_bow = cvec.transform(corpus)\n",
    "    print(len(train_bow.toarray()))\n",
    "    return train_bow.toarray() , cvec.get_feature_names() #train_bow.toarray() \n",
    "    \n",
    "## LDA GLOVE CORPUS\n",
    "\n",
    "\n",
    "def knn(W, x, k):\n",
    "    # The added 1e-9 is for numerical stability\n",
    "    cos = nd.dot(W, x.reshape((-1,))) / (\n",
    "        (nd.sum(W * W, axis=1) + 1e-9).sqrt() * nd.sum(x * x).sqrt())\n",
    "    topk = nd.topk(cos, k=k, ret_typ='indices').asnumpy().astype('int32')\n",
    "    return topk, [cos[i].asscalar() for i in topk]\n",
    "\n",
    "def get_similar_tokens(query_token, k, embed):\n",
    "    topk, cos = knn(embed.idx_to_vec,\n",
    "                    embed.get_vecs_by_tokens([query_token]), k+1)\n",
    "    list_sym = [] \n",
    "    for i, c in zip(topk[1:], cos[1:]):  # Remove input words\n",
    "        #print('cosine sim=%.3f: %s' % (c, (embed.idx_to_token[i])))\n",
    "        list_sym.append(embed.idx_to_token[i])\n",
    "        \n",
    "    return   list_sym\n",
    "\n",
    "df_new_test['clean_content_ner'] =  df_new_test.progress_apply(lambda x: ner_process(x['Title'] + ' ' + x['Content']), axis=1)\n",
    "df_new_test['clean_content_prep'] = df_new_test['clean_content_ner'].progress_apply(prep_data) \n",
    "df_new_test['clean_content_prep_last'] =  df_new_test['clean_content_prep'].progress_apply(lambda x : word_tokenize(x))\n",
    "df_new_test['clean_content_prep_pos_n'] =  df_new_test['clean_content_prep'].progress_apply(noun_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20496bf7",
   "metadata": {},
   "source": [
    "## Read File Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2db191a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7dc48083e944713bbec9389a7f46297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff1957da99448bda9e45b21404068ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128cbeb122a0499ba6a8e5c77978d86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/967 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: more-itertools in /home/nick/envDITP/lib/python3.8/site-packages (8.12.0)\n",
      "Before 963\n",
      "Final 658\n"
     ]
    }
   ],
   "source": [
    "path = '/home/nick/DITP/trade-midas/ditp3_run_test/'\n",
    "\n",
    "#econ \n",
    "econ_dict =  pd.read_csv(f'{path}/econ_dict_corpus.csv', index_col=False)\n",
    "econ_dict.Key_word = econ_dict.Key_word.progress_apply(prep_data)\n",
    "econ_dict.Relate_word = econ_dict.Relate_word.progress_apply(prep_data)\n",
    "\n",
    "\n",
    "#related product \n",
    "relate_product = pd.read_csv(f'{path}/related_product_corpus.csv')\n",
    "relate_key = relate_product['keyword'].to_list()\n",
    "relate_product.keyword = relate_product.keyword.progress_apply(prep_data)\n",
    "\n",
    "\n",
    "rp =  relate_product.keyword.to_list()\n",
    "#all_p = all_product.keyword.to_list()\n",
    "#location_l = location.keyword.to_list()\n",
    "econ_d_key  =  econ_dict.Key_word.to_list()\n",
    "econ_d_related  =  econ_dict.Relate_word.to_list()\n",
    "\n",
    "\n",
    "econ_all = econ_d_key + econ_d_related\n",
    "econ_all =  list(set(econ_all))\n",
    "econ_all = list(map( lambda x : x.lower(), econ_all))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "! pip install more-itertools\n",
    "from functools import reduce\n",
    "import more_itertools ## reduct list nest to sigle list \n",
    "rp = list(map( lambda x : x.lower() , rp))\n",
    "\n",
    "\n",
    "\n",
    "rp =  list(filter(lambda x : len(x) > 2 , rp))\n",
    "print('Before' , len(rp))\n",
    "rp_new =  [ i.split(' ')  if len(i.split(' ')) > 1  else i  for i in rp ]\n",
    "rp_new = list(more_itertools.collapse(rp_new))\n",
    "rp_new =  list(set(rp_new))\n",
    "rp_new =  list(filter( lambda x : len(x) > 3,rp_new )) + ['oil'] \n",
    "print('Final',len(rp_new))\n",
    "\n",
    "\n",
    "except_list_rp = ['mens','woman','swim', 'basic','semi','fats','long','sale','extra','room','shorts','mixed','queen'\n",
    "                 ,'hair','form','base','cuts','cars','type','root','prcs','swim','thai','women','japan','acid','beds'\n",
    "                 ,'sound','tree','clad','parts','super','clip'] +  ['wind','block','sheets','star','dried'\n",
    "                       ,'roots','example','shock','slips','cubeb','white','brown'\n",
    "              ,'broken','coin','aircraft','preparations','long','ephedras','room','short','special','mixed','overalls'\n",
    "              ,'pipes','ensembles' ,'capacity','aloe','japan' ,'flax','women','indonesian' ,'elemi','leaves','plugs'\n",
    "              ,'root','gear','columns' , 'swim' ,'type' ,'boys','lead','sassafras','semi','basic','bran','drms','load','goods'\n",
    "               ,'prcs','spokes','kind','shower','sheet','broke','purpose','base','road','shorts','super','tools','toilet','part'\n",
    "               ,'logs','thai','tons','chinese','care','photographs','latex','vera','holy','form','sound','sage','bird','forms',\n",
    "               'wall','hair','logs'\n",
    "\n",
    "              ]\n",
    "\n",
    "rp_new_clean =  [ i  for i in rp_new if i not in   except_list_rp ]\n",
    "\n",
    "rp_new_clean = list(set(rp_new_clean))\n",
    "\n",
    "\n",
    "#### Econ dictionary \n",
    "\n",
    "from functools import reduce\n",
    "import more_itertools ## reduct list nest to sigle list \n",
    "econ_all1 =  [  iu.split(' ') if len(iu.split()) > 1 else iu for iu in  econ_all  ]\n",
    "econ_all1 = list(more_itertools.collapse(econ_all1))\n",
    "econ_all1 =  list(set(econ_all1))\n",
    "\n",
    "\n",
    "\n",
    "except_word  = ['say','zero','numbers','terms','mac','boom','ppp','account','asian','path','simple','rate','null'\n",
    "    ,'mode','long','zone','exit','repo','area','man','areas','common','mean','mixed','spot','tick','hot'\n",
    ",'base','number','best','entry','bust','plaza','point','non','okun','neo','hard','tiger','way','the','paris','big','plan',\n",
    "'third','normal','fine','job','and','command','soft','aid','work','game','ngo','search','last','nairu','full','new','homo'\n",
    ",'alan','sum','euro','john','club','government','bills','out','good','beta','lbo','open','all','random','right','for'\n",
    " ,'closed','current','security','environmental','']\n",
    "econ_all2 =  [ i   for i in econ_all1 if len(i) > 2 and i not in except_word  ]\n",
    "\n",
    "all_word  =  econ_all2 \n",
    "new_all_word =   list(filter(lambda x :  len(x) >  2 , all_word ))\n",
    "new_all_word =  [ i.lower() for i in new_all_word ]\n",
    "\n",
    "\n",
    "\n",
    "## related word LDA \n",
    "df_lda =  pd.read_csv('./lda_keyword_phase2.csv')\n",
    "de_set = df_lda['keyword_lda'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cbcaeb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790ac91141f242969552c326b11f8b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe09ac86df94fb393769c1f76f62d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ab3625a99b4a1dbdb906fe586c63ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "re_product_list = []\n",
    "for io in   tqdm(df_new_test.to_dict('record')):\n",
    "    dic_cont = [ i.lower() for i in  io['clean_content_prep_pos_n']] #\n",
    "    \n",
    "    #print(dic_cont)\n",
    "#     print(rp_new_clean)\n",
    "    xxee = list(filter( lambda r :  r in  rp_new_clean , dic_cont))\n",
    "    #print(xxee)\n",
    "    re_product_list.append(' '.join(xxee))\n",
    "    \n",
    "    \n",
    "    \n",
    "econ_list = []\n",
    "for ii in   tqdm(df_new_test.to_dict('record')):\n",
    "    dic_cont =  ii['clean_content_prep_last']               \n",
    "    xxdd = list(filter( lambda x :  x in new_all_word , dic_cont)) \n",
    "    econ_list.append(' '.join(xxdd))\n",
    "    \n",
    "    \n",
    "lda_list = []\n",
    "for ii in   tqdm(df_new_test.to_dict('record')):\n",
    "    dic_cont =  ii['clean_content_prep_last']               \n",
    "    xxlda = list(filter( lambda x :  x in de_set , dic_cont)) \n",
    "    lda_list.append(' '.join(xxlda))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9f7e349e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "650\n",
      "128\n",
      "1\n",
      "569\n",
      "128\n",
      "1\n",
      "881\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "#Economy dictionary\n",
    "\n",
    "econ_array,econ_feature = count_vector(new_all_word,econ_list)\n",
    "df_econ =pd.DataFrame( econ_array , columns = econ_feature, index = range(len(econ_list)+1))\n",
    "try:\n",
    "    df_econ.drop(columns  = [''] , inplace=True)\n",
    "except: \n",
    "    pass\n",
    "df_econ['sum_unique'] = df_econ.replace(0, np.nan).notnull().sum(axis= 1)#.notnull().count(axis=1)\n",
    "df_econ['sum_total'] =  df_econ.iloc[:, : -1 ].sum(axis= 1)\n",
    "\n",
    "df_econ.drop(index= 0 , inplace=True)\n",
    "df_econ_index  =  df_econ.index.to_list()\n",
    "df_econ_index =  [i-1 for i in df_econ_index ]\n",
    "df_econ.index = df_econ_index\n",
    "\n",
    "q3 = 7\n",
    "df_econ1 = df_econ[df_econ['sum_unique'] >= q3]\n",
    "\n",
    "# related propuct  \n",
    "product_array,product_feature = count_vector(rp_new_clean,re_product_list)\n",
    "df_product_related =pd.DataFrame( product_array , columns = product_feature, index = range(len(re_product_list)+1))\n",
    "try:\n",
    "    df_product_related.drop(columns = [''], inplace=True)\n",
    "except: \n",
    "    pass\n",
    "df_product_related['sum_unique'] = df_product_related.replace(0, np.nan).notnull().sum(axis= 1)#.notnull().count(axis=1)\n",
    "df_product_related['sum'] =  df_product_related.iloc[:, : -1 ].sum(axis= 1)\n",
    "\n",
    "\n",
    "df_product_related.drop(index=0 , inplace=True)\n",
    "df_product_related_index  =  df_product_related.index.to_list()\n",
    "df_product_related_index =  [i-1 for i in df_product_related_index ]\n",
    "df_product_related.index = df_product_related_index\n",
    "\n",
    "q3_related = 2\n",
    "df_product_related1 = df_product_related[df_product_related['sum'] >= q3_related]\n",
    "\n",
    "## LDA key word\n",
    "lda_array,product_feature_lda = count_vector(de_set,lda_list)\n",
    "df_lda_related =pd.DataFrame( lda_array , columns = product_feature_lda, index = range(len(lda_list)+1))\n",
    "df_lda_related.drop(columns = [''], inplace=True,errors='ignore')\n",
    "df_lda_related['sum_unique'] = df_lda_related.replace(0, np.nan).notnull().sum(axis= 1)#.notnull().count(axis=1)\n",
    "df_lda_related['sum_total'] =  df_lda_related.iloc[:, : -1 ].sum(axis= 1)\n",
    "\n",
    "df_lda_related.drop( index=0, inplace=True )\n",
    "df_lad_related_index  =  df_lda_related.index.to_list()\n",
    "df_lad_related_index =  [i-1 for i in df_lad_related_index ]\n",
    "df_lda_related.index = df_lad_related_index\n",
    "\n",
    "q3_lda =  20\n",
    "df_lda_related1 = df_lda_related[df_lda_related['sum_unique'] >=q3_lda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "122c5361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_econ['sum_unique'][15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bad3df4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 2, 12)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 15\n",
    "df_econ['sum_unique'][i],df_product_related['sum_unique'][i],df_lda_related['sum_unique'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4032b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA realted corpus\n",
    "# from mxnet import nd\n",
    "# from mxnet.contrib import text\n",
    "# glove_6b50d = text.embedding.create(\n",
    "#     'glove', pretrained_file_name='glove.840B.300d.txt')\n",
    "\n",
    "all1 = df_econ1[['sum_unique','sum_total']].merge(df_product_related1[['sum_unique','sum']], right_index=True, left_index=True  )\n",
    "all2 = all1.merge(df_lda_related1[['sum_unique','sum_total']],  right_index=True, left_index=True )\n",
    "#df_product_related\n",
    "x2 = df_new_test[['Content','Verified by\\nDITP']].merge(all2, how = 'left',right_index=True, left_index=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4214dfc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 8)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2[(x2['Verified by\\nDITP'] == 'yes') & (x2['sum_unique_x'].isna())  ].shape #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "78016e6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_425435/1207805328.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Verified by\\nDITP'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'yes'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sum_unique'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 101  yes yes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 1     yes no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x1' is not defined"
     ]
    }
   ],
   "source": [
    "x1[(x1['Verified by\\nDITP'] == 'yes') & (x1['sum_unique'] ==0 ) ].shape\n",
    "\n",
    "# 101  yes yes \n",
    "# 1     yes no \n",
    "\n",
    "# no  yes  14  high gain  \n",
    "# no no    6 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f0f2aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = df_new_test[['Content','Verified by\\nDITP']].merge(df_econ[['sum_total','sum_unique']], how = 'left',right_index=True, left_index=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6021b5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 4)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x2.replace(np.nan ,  0, inplace=True)\n",
    "x2[(x2['Verified by\\nDITP'] == 'no') & (x2['sum_unique'] !=0 ) ].shape\n",
    "# total  yes =  102 \n",
    "# yes yes =  63\n",
    "# yes  no = 39 \n",
    "# no no   2\n",
    "# no yes   18 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40e40cec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_econ0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_383247/4198429499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# intersaction  with  3 domain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m all1 = df_econ0[['sum_unique','sum_total']].merge(df_product_related0[['sum_unique','sum']],\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                   right_index=True, left_index=True  )\n\u001b[1;32m      4\u001b[0m \u001b[0mall2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_lda_related0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sum_unique'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sum_total'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_econ0' is not defined"
     ]
    }
   ],
   "source": [
    "# intersaction  with  3 domain \n",
    "all1 = df_econ0[['sum_unique','sum_total']].merge(df_product_related0[['sum_unique','sum']],\n",
    "                                                  right_index=True, left_index=True  )\n",
    "all2 = all1.merge(df_lda_related0[['sum_unique','sum_total']],  right_index=True, left_index=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9ced04c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_383247/233421061.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m all2.rename(columns={'sum_unique_x':'sum_unique_econ',\n\u001b[0m\u001b[1;32m      2\u001b[0m                      \u001b[0;34m'sum_total_x'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'sum_total_econ'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                      \u001b[0;34m'sum_unique_y'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'sum_unique_relate_pro'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      \u001b[0;34m'sum'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'sum_total_relate_pro'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      \u001b[0;34m'sum_unique'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'sum_unique_lda'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all2' is not defined"
     ]
    }
   ],
   "source": [
    "all2.rename(columns={'sum_unique_x':'sum_unique_econ',\n",
    "                     'sum_total_x':'sum_total_econ',\n",
    "                     'sum_unique_y' : 'sum_unique_relate_pro',\n",
    "                     'sum' : 'sum_total_relate_pro',\n",
    "                     'sum_unique' : 'sum_unique_lda',\n",
    "                     'sum_total_y' : 'sum_total_lda'\n",
    "                    }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b90e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
